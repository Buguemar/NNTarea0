{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Redes Neuronales </H3>\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<p align='center'> Alfredo Silva Celpa   201373511-8 </p>\n",
    "<p align='center'> Margarita Bugueño Pérez   201373510-K </p>\n",
    "\n",
    "**Temas**  \n",
    "* NNs por dentro: *back-propagation from scratch*.\n",
    "* Principales hiperparámetros de *back propagation*\n",
    "* Introducción a keras\n",
    "* Verificación numérica de las derivadas implementadas.\n",
    "\n",
    "** Formalidades **  \n",
    "* Equipos de trabajo de: 2 personas (*cada uno debe estar en condiciones de responder preguntas sobre cada punto del trabajo realizado*)\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega: 30 de Marzo.\n",
    "* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<francisco.mena.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<jnancu@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea0-INF395-I-2018]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "\n",
    "#### Paquetes instalación\n",
    "Para poder trabajar en el curso se necesitará instalar librerías para Python, por lo que se recomienda instalarlas a través de *anaconda* (para Windows y sistemas Unix) en un entorno virtual, donde podrán elegir su versión de Python. Se instalarán librerías como __[*sklearn*](http://scikit-learn.org/stable/)__, una librería simple y de facil acceso para *data science*, __[*keras*](https://keras.io/)__ en su versión con GPU (para cálculo acelerado a través de la tarjeta gráfica), además de que ésta utiliza como *backend* *TensorFlow* o *Theano*, por lo que habrá que instalar alguno de éstos, además de las librerías básicas de *computer science* como *numpy*, *matplotlib*, *pandas*, además de claramente *jupyter*.\n",
    "\n",
    "* __[Descargar anacona](https://www.anaconda.com/download/#linux)__ \n",
    "\n",
    "* Luego de instalar Anaconda y tenerla en el *path* de su computador crear un entorno virtual: \n",
    "```\n",
    "conda create -n redesneuronales python=version\n",
    "```\n",
    "\n",
    "con *version*, la version de Python que desea utilizar. Si está en Windows, se recomienda Python 3 debido a dependencias con una de las librerías a utilziar.\n",
    " \n",
    "* Acceder al ambiente creado \n",
    "```\n",
    "source activate redesneuronales\n",
    "```\n",
    "\n",
    "* Instalar los paquetes a utilizar\n",
    "```\n",
    "conda install jupyter sklearn numpy pandas matplotlib keras-gpu tensorflow-gpu \n",
    "```\n",
    "\n",
    "* Para salir del entorno\n",
    "```\n",
    "source deactivate redesneuronales\n",
    "```\n",
    "<hr style=\"height:1px;border:none\"/>\n",
    "\n",
    "\n",
    "La tarea se divide en cuatro secciones:\n",
    "\n",
    "[1.](#primero)   Back-propagation (BP) from *Scratch*   \n",
    "[2.](#segundo)   Comparar back-propagation (BP) de Keras  \n",
    "[3.](#tercero)   Verificación numérica del gradiente para una componente  \n",
    "[4.](#cuarto)   Implementar momentum como variante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Back-propagation (BP) from *Scratch*\n",
    "\n",
    "BP (Back-propagation) es sin duda el paradigma dominante para entrenar redes neuronales *feed-forward*. En\n",
    "redes grandes, diseñadas para problemas reales, implementar BP eficientemente puede ser una tarea delicada\n",
    "que puede ser razonable delegar a una librerı́a especializada. Sin embargo, construir BP *from scratch* es muy\n",
    "útil con fines pedagógicos.\n",
    "\n",
    "$$ w^{(t+1)} \\leftarrow w^{(t)} - \\eta \\nabla_{w^{(t)}} Loss $$\n",
    " \n",
    "> a) Escriba un programa que permita entrenar una red FF con una arquitectura fija de 2 capa ocultas (con 32 neuronas en la primera capa y 16 en la segunda) y $K$ neuronas de salida, sin usar librerı́as, excepto eventualmente *numpy* para implementar operaciones básicas de algebra lineal. Por simplicidad, asuma funciones de activacion y error (*loss function*) diferenciables o subdiferenciables, además de tener la misma función de activación para las 2 capas ocultas. Adapte la arquitectura para un problema de clasificación de 3 clases, es decir la función de activación para la capa de salida debe ser **softmax** con número de neuronas $K$=3. Escriba funciones para:  \n",
    "* (i)  implementar el *forward pass*  \n",
    "* (ii) implementar el *backward pass*  \n",
    "* (iii) implementar la rutina principal de entrenamiento, adoptando, por simplicidad, la variante cíclica aleatorizada de SGD (un ejemplo a la vez, pero iterando cíclicamente sobre una configuración aleatoria del conjunto de entrenamiento) con una tasa de aprendizaje fija de 0.1 y número de ciclos fijos (*epochs*).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Inicializar en algún modo;\n",
    "#while criterio de parada insatisfecho do\n",
    "#for Cada ejemplo (xi, yi ) do\n",
    "#  Forward pass (xi, yi );\n",
    "#  Medir error E = E(xi, yi );\n",
    "#  Backward pass E;\n",
    "\n",
    "#se debe entrenar por batches. Tal que, se pasan en forwarding un batch completo (cada ejemplo permite computar \n",
    "# el error, se almacen en un arreglo (?) y se procede con el backwarding que sólo se ejecutará para obtener el \n",
    "# gradiente de cada error y almacenarlo en un arreglo nuevamente para, al final del batch realizar el backprop \n",
    "# con actualizacion de pesos)\n",
    "# nuevamente realizar este procedimiento con el siguiente batch de entrenamiento y fin :D\n",
    "def sigmoidal(x):\n",
    "    if x>0:\n",
    "        return 1/(1+ np.exp(-x))\n",
    "    else:\n",
    "        return 1/(1+ np.exp(-(1e-20)))\n",
    "\n",
    "def d_sigmoidal(x):\n",
    "    return sigmoidal(x)*(1-sigmoidal(x))\n",
    "\n",
    "#donde x es un vector\n",
    "def softmax(x):\n",
    "    salida=[] \n",
    "    suma=0.0\n",
    "    for elemento in range(len(x)):\n",
    "        suma+=np.exp(x[elemento])\n",
    "    print (suma)\n",
    "    for elemento in range(len(x)):\n",
    "        temp=np.exp(x[elemento])/suma_x\n",
    "        salida.append(temp)\n",
    "    print (salida)\n",
    "    return salida\n",
    "\n",
    "def d_softmax(x):\n",
    "    return softmax(x)*(np.subtract(1,softmax(x)))\n",
    "\n",
    "def error_mse(ypred, ytrue):\n",
    "    # no se si debamos imlementarlo como 1/2*(ytrue-ypred)**2\n",
    "    return (0.5)*((np.subtract(ytrue,ypred))**2)\n",
    "\n",
    "def d_error_mse(ypred, ytrue):\n",
    "    # si es asi, entonces se omitiría el 2 aquí\n",
    "    return (np.subtract(ytrue,ypred))\n",
    "\n",
    "def error_CE(ypred, ytrue):\n",
    "    return (np.add((ytrue*ypred),(np.subtract(1,ytrue)*(np.subtract(1,ypred)))))\n",
    "\n",
    "def d_error_CE(ypred, ytrue):\n",
    "    return (np.subtract(ytrue,ypred))/(ypred*(np.subtract(1,ypred)))\n",
    "\n",
    "\n",
    "#definir el modelo y su arquitectura\n",
    "#total capas= capas ocultas + capa de salida\n",
    "def size_layers(total_capas,input_size,n_capa_o1, n_capa_o2, n_capa_out):\n",
    "    sizes=np.arange(total_capas+1)\n",
    "    sizes[0]=input_size\n",
    "    sizes[1]=n_capa_o1\n",
    "    sizes[2]=n_capa_o2    \n",
    "    sizes[3]=n_capa_out\n",
    "    return sizes\n",
    "\n",
    "def iniciar_estructuras(total_capas,tamanios_arquit):\n",
    "    #vector sallida de cada capa, vector derivada de sallida respecto a w, vector error en la salida de cada capa\n",
    "    salida_l=[]\n",
    "    da_salida_l=[]\n",
    "    e_salida_l=[]\n",
    "    for i in np.arange(total_capas+1):\n",
    "        salida_l.append(np.zeros(tamanios_arquit[i+1]))\n",
    "            \n",
    "    da_salida_l=salida_l.copy()\n",
    "    e_salida_l=salida_l.copy()    \n",
    "    #matrices de pesos originales, matrices errores de pesos\n",
    "    #matrices de peso inicializadas como 1\n",
    "    m_pesos=[]\n",
    "    for i in np.arange(total_capas):\n",
    "        #print (np.ones((tamanios_arquit[i],tamanios_arquit[i+1])))\n",
    "        m_pesos.append(np.ones((tamanios_arquit[i],tamanios_arquit[i+1])))        \n",
    "    #m_pesos.append(m_pesos_c1_c2)\n",
    "    #m_pesos.append(m_pesos_c2_c3)\n",
    "    #matrices de errores en los pesos inicializadas como 0\n",
    "    e_m_pesos=[]\n",
    "    for i in np.arange(total_capas):\n",
    "        #print (np.zeros((tamanios_arquit[i],tamanios_arquit[i+1])))\n",
    "        e_m_pesos.append(np.zeros((tamanios_arquit[i],tamanios_arquit[i+1])))  \n",
    "      \n",
    "    return salida_l, da_salida_l, e_salida_l, m_pesos, e_m_pesos    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forwarding(total_capas, vector_x, salida_l, da_salida_l,m_pesos, vector_arquit):\n",
    "    vector=vector_x.copy() #se modifica en cada capa\n",
    "    salida_l[-1]=vector.copy()\n",
    "    ##################333333######DERIVADAAAA\n",
    "    da_salida_l[-1]=vector.copy()\n",
    "    ##################### DERIVADA\n",
    "    print (vector)\n",
    "    for capa in range(total_capas-1):\n",
    "        salida_capa=[]\n",
    "        d_salida_capa=[]\n",
    "        for neurona in range(vector_arquit[capa+1]):\n",
    "            #pesos como vector columna de m_pesos\n",
    "            w=(m_pesos[capa].T[neurona]).copy() #vector pesos hacia neurona desde las xs inputs (vector dimesion entrada a capa)\n",
    "            temp= np.dot(vector,w)\n",
    "            temp=sigmoidal(temp)\n",
    "            temp2=d_sigmoidal(temp)\n",
    "            salida_capa.append(temp)\n",
    "            d_salida_capa.append(temp2)\n",
    "        salida_l[capa]=salida_capa.copy()\n",
    "        da_salida_l[capa]=d_salida_capa.copy()\n",
    "        vector=salida_capa.copy()\n",
    "    print (vector)\n",
    "    salida_last_t=[]\n",
    "    d_salida_last=[] \n",
    "    for neurona in range(vector_arquit[-1]): #0,1,2               \n",
    "        #pesos como vector columna de m_pesos\n",
    "        w=(m_pesos[-1].T[neurona]).copy() #vector pesos entre ult. capa oculta y la capa out (dimesion largo de capa anterior)\n",
    "        temp= np.dot(vector,w) #elemento i-esimo entrada a softmax (de la salida de la capa)\n",
    "        salida_last_t.append(temp) #vector de w*a    \n",
    "    salida_last=softmax(salida_last_t)\n",
    "    d_salida_last=d_softmax(salida_last_t)\n",
    "    salida_l[-2]=salida_last.copy()\n",
    "    da_salida_l[-2]=d_salida_last.copy()\n",
    "    vector=salida_last.copy()\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwarding(total_capas, vector_x, vector_y, tipo_error ,tasa, salida_l, da_salida_l,m_pesos, vector_arquit):\n",
    "    #for neurona in range(vector_arquit[-1]):        \n",
    "        if tipo_error==\"mse\":\n",
    "            error_EA=d_error_mse(salida_l[-2], vector_y)\n",
    "        else:\n",
    "            error_EA=d_error_CE(salida_l[-2], vector_y)\n",
    "        error_AW=np.dot(da_salida_l[-2],salida_l[-3])\n",
    "        error_EW=np.dot(error_EA,error_AW)\n",
    "        e_salida_l[total_capas-1]=error_EA.copy() #es el triangulo (Delta may.)\n",
    "        print (error_EW)\n",
    "        #corrección de pesos que debiese ser por batch \n",
    "        auxiliar=[]\n",
    "        for columna in m_pesos[total_capas-1].T:\n",
    "            columna=columna-(tasa*error_EW)\n",
    "            auxiliar.append(columna)\n",
    "        m_pesos[total_capas-1]=auxiliar.T\n",
    "        for capa in np.arange(total_capas-2,-1,-1): #se mueve en capa 1 y 0\n",
    "            error_EA_s=np.sum(e_salida_l[capa+1]) #suma el vector EA calculado en la capa 2 (igual para todos)\n",
    "            error_aux=[]\n",
    "            for neurona in range(vector_arquit[capa+1]):\n",
    "                error_EW_s=error_EA_s*da_salida_l[capa]*salida_l[capa-1]\n",
    "                #corregir pesos\n",
    "                auxiliar2=[]\n",
    "                for columna in m_pesos[capa].T:\n",
    "                    columna=columna-(tasa*error_EW_s)\n",
    "                    auxiliar2.append(columna)\n",
    "                m_pesos[capa]=auxiliar2.T\n",
    "                error_aux.append(error_EA_s)\n",
    "            e_salida_l[capa]=error_aux.copy()\n",
    "        return m_pesos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN_2ocultas:\n",
    "    def __init__(self, total_capas, tamanios_arquit):\n",
    "        self.n_capa_in=tamanios_arquit[0]\n",
    "        self.n_capa_1=tamanios_arquit[1]\n",
    "        self.n_capa_2=tamanios_arquit[2]\n",
    "        self.n_capa_out=tamanios_arquit[3]\n",
    "        self.capas=total_capas\n",
    "        self.salidas_l, self.da_salida_l, self.e_salida_l, self.m_pesos, self.e_m_pesos=iniciar_estructuras(total_capas,tamanios_arquit)\n",
    "\n",
    "    def training(Xtrain, Ytrain, salidas_l, da_salida_l, e_salida_l, m_pesos, m_error, epoch):\n",
    "        #X_aux = Xtrain.copy()\n",
    "        select = numpy.random.choice(Xtrain, size=epoch, replace=False)\n",
    "        for xs in select:\n",
    "            \n",
    "            forwarding()\n",
    "        #for x in Xtrain:\n",
    "        #  Forward pass (xi, yi );\n",
    "        #  Medir error E = E(xi, yi );\n",
    "        #ojo! backwarding-SGD por batch (este caso es batch tamaño 1)\n",
    "        #  Backward pass E;\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([64, 32, 16,  3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=size_layers(3,64,32,16,3)\n",
    "x=NN_2ocultas(3, t)\n",
    "x.n_capa_out\n",
    "(x.m_error[2])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(1,-1,-1):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230.485902158\n",
      "[0.23688281808991013, 0.032058603280084988, 0.087144318742032573, 0.64391425988797235]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23688281808991013,\n",
       " 0.032058603280084988,\n",
       " 0.087144318742032573,\n",
       " 0.64391425988797235]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[4,2,3,5]\n",
    "\n",
    "softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.absolute((np.subtract(a,b))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b) Escriba una función que permita hacer predicciones mediante la red FF definida anteriormente, sin usar librerı́as, excepto eventualmente *numpy*. Escriba una función vectorizada que implemente el forward pass sobre un conjunto de $n_{t}$ ejemplos, además de implementar la función de decisión, que a través de la salida de la red prediga el valor categórico de la clase (1, 2 o 3).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> c) Demuestre que sus programas funcionan en un problema de clasificación. Para esto utilice el dataset **iris**, disponible a través de la librería __[*sklearn*](http://scikit-learn.org)__, el cual corresponde a la clasificación de distintos tipos de plantas de iris (3 clases) mediante 4 características reales continuas específicas de la planta, deberá entrenar (ajustar) los pesos de la red para realizar la tarea encomendada, variando las funciones de error (*loss*) entre *categorical cross entropy* y *mean squared error*, además de variar las funciones de activación para las 2 capas ocultas entre  ReLU (Rectifier Linear Unit) y la función logística (*sigmoid*). Especifique explícitamente las funciones anteriores, así como sus gradientes. Recuerde que debe transformar las etiquetas usando *one hot vectors*.\n",
    "<div class=\"alert alert-block alert-info\">Es una buena práctica el normalizar los datos antes de trabajar con el modelo</div>\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "X_train,y_train = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "#transform target to one hot vector\n",
    "import keras\n",
    "y_onehot = keras.utils.to_categorical(y_train)\n",
    "```\n",
    "Para evaluar los resultados, construya un gráfico correspondiente al error de clasificación versus número\n",
    "de epochs, utilizando sólo el conjunto de entrenamiento (el objetivo de esta sección es familiarizarse\n",
    "con el algoritmo BP, no encontrar la mejor red). Grafique también la evolución de la función objetivo utilizada para el entrenamiento. Además de reportar el tiempo de entrenamiento mediante el algoritmo implementado.  \n",
    "Por último, para alguna configuración elegida, reporte la matriz de confusión mediante el uso de librerías como *sklearn* o *keras*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "### 2. Comparar back-propagation (BP) de Keras\n",
    "\n",
    "Keras es una de las librerı́as más populares para desarrollar nuevos modelos de redes neuronales o implementar eficientemente modelos conocidos con fines prácticos, puesto que ofrece una interfaz para poder trabajar de una manera mucho mas simple además de permitir también el manejo de configuraciones mas específicas.  \n",
    "Como actividad pedagógica ahora se les pide comparar el algoritmo implementado por ustedes con el de alto nivel de la librería __[keras](https://keras.io/)__ . Se les pedirá comparar sobre el mismo dataset con la misma arquitectura utilizada anteriormente, es decir, dos capas ocultas (con 32 y 16 neuronas respectivamente), 3 neuronas en la capa de salida con función de activación softmax, optimizador Gradiente Descentente (GD) con tasa de aprendizaje fija.\n",
    "\n",
    "<img src=\"https://i.imgur.com/hUjFUDU.png\" width=\"40%\" height=\"40%\" />\n",
    "\n",
    "\n",
    "> a) Defina, a través de la interfaz de keras, la arquitectura de la red, con las funciones de activación para comparar con la sección anterior.\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation=\"sigmoid or relu\"))\n",
    "model.add(Dense(16, activation=\"sigmoid or relu\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "```\n",
    "\n",
    "> b) Defina, a través de la interfaz de keras, el optimizador de la red, en conjunto con la función de error, para poder comparar con la sección anterior.\n",
    "```python\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.1),loss=\"categorical_crossentropy or mse\", metrics=[\"accuracy\"])\n",
    "```\n",
    "\n",
    "> c) Entrene (ajuste) los pesos de la red definida mediante keras, reportando los mismos gráficos de la sección anterior para poder comparar. Si hay diferencias en la convergencia del algoritmo ¿A qué podría deverse? si hay una gran diferencia en los tiempos de entrenamiento ¿A qué podría deverse?\n",
    "```python\n",
    "model.fit(X_train, y_onehot, epochs=100, batch_size=1, verbose=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tercero\"></a>\n",
    "### 3. Verificación numérica del gradiente para una componente\n",
    "\n",
    "En esta sección deberá verificar numéricamente el gradiente para los parámetros del modelo (que en este caso son los pesos de la red), que hasta ahora a definido de manera analítica en su programa, por ejemplo la derivada de $x^2$ es $2x$. Ahora deberá verificar estos cálculos usando la definición de gradiente.\n",
    "\n",
    "$$ \\nabla_{w} Loss = \\lim_{\\epsilon \\rightarrow 0} \\frac{Loss(w+ \\epsilon)-Loss(w)}{\\epsilon} $$\n",
    "\n",
    "Debido a que el *forward propagation* es relativamente fácil de implementar, se puede confiar en que se realizó de manera correcta, por lo que el cómputo del error (*loss*) debería ser correcto. Esto significa que podemos verificar el gradiente o la derivada analítica del error $\\frac{\\partial Loss}{\\partial w}$ comprobando que el resultado obtenido es similar (dentro de una tolerancia numérica, por ejemplo $10^6$) al valor que obtenemos aplicando la fórmula anterior. Naturalmente interpretaremos $\\lim_{\\epsilon \\rightarrow 0}$ como un valor \"*suficientemente pequeño*\" de $\\epsilon$.\n",
    "\n",
    "\n",
    "> a) Para un peso escogido aleatoriamente entre la primera capa de la red (*input*) y la primera capa oculta, calcule el valor del gradiente de la función de error para ambas funciones utilizadas (ayúdese mediante las funciones de *backward pass* implementadas anteriormente), luego compare y verifique con el valor numérico del gradiente mediante el procedimiento explicado anteriormente.\n",
    "\n",
    "> b) Vuelva a verificar el valor del gradiente para otros dos pesos escodigos aleatoriamente en la primera operación de la red. Compare y concluya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cuarto\"></a>\n",
    "### 4. Implementar *momentum* como variante\n",
    "\n",
    "En esta sección deberá construir, sin usar librerı́as, excepto eventualmente *numpy* para implementar operaciones básicas de algebra lineal, una variante del programa definido anteriormente ([sección 1](#primero)) que entrene la red utilizando *momentum* clásico.\n",
    "\n",
    "$$ v^{(t+1)} \\leftarrow \\mu v^{(t)} - \\eta \\nabla_{w^{(t)}} Loss \\\\\n",
    "w^{(t+1)} \\leftarrow w^{(t)} + v^{(t+1)}\n",
    "$$\n",
    "\n",
    "> *Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013, February). On the importance of initialization and momentum in deep learning. In International conference on machine learning (pp. 1139-1147).*\n",
    "\n",
    "\n",
    "Demuestre que su programa funciona en el mismo problema de clasificación presentado anteriormente, para esto, además deberá construir un gráfico de la función de error o pérdida (*loss*) *vs* el número de *epochs* y comentar/analizar la convergencia. ¿Es una mejora significativa? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
